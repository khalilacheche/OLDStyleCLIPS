{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4J1lmK2Viqey"
      },
      "source": [
        "# Text-Guided Editing of Images (Using CLIP and StyleGAN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILQOp5_9bI1o",
        "outputId": "9b454346-e134-426a-c257-3133f272e957"
      },
      "outputs": [],
      "source": [
        "#@title Setup (may take a few minutes)\n",
        "import os\n",
        "os.chdir('/content')\n",
        "CODE_DIR = 'encoder4editing'\n",
        "\n",
        "!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html \n",
        "\n",
        "!git clone https://github.com/omertov/encoder4editing.git $CODE_DIR\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "\n",
        "!pip install ftfy regex tqdm \n",
        "!pip install git+https://github.com/openai/CLIP.git \n",
        "\n",
        "from argparse import Namespace\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Cf_0JNiGHpP",
        "outputId": "a990039a-6ec8-4ec6-e9e1-62c7bf4ad727"
      },
      "outputs": [],
      "source": [
        "#@title e4e setup\n",
        "os.chdir(f'./{CODE_DIR}')\n",
        "from utils.common import tensor2im\n",
        "from models.psp import pSp\n",
        "from gdown import download as drive_download\n",
        "from google.colab import files\n",
        "drive_download(\"https://drive.google.com/uc?id=1O8OLrVNOItOJoNGMyQ8G8YRTeTYEfs0P\", \"/content/encoder4editing/e4e_ffhq_encode.pt\", quiet=False)\n",
        "experiment_type = 'ffhq_encode'\n",
        "\n",
        "EXPERIMENT_ARGS = {\n",
        "        \"model_path\": \"e4e_ffhq_encode.pt\"\n",
        "    }\n",
        "EXPERIMENT_ARGS['transform'] = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "resize_dims = (256, 256)\n",
        "\n",
        "model_path = EXPERIMENT_ARGS['model_path']\n",
        "ckpt = torch.load(model_path, map_location='cpu')\n",
        "opts = ckpt['opts']\n",
        "# pprint.pprint(opts)  # Display full options used\n",
        "# update the training options\n",
        "opts['checkpoint_path'] = model_path\n",
        "opts= Namespace(**opts)\n",
        "net = pSp(opts)\n",
        "net.eval()\n",
        "net.cuda()\n",
        "print('Model successfully loaded!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "wnuqNaPWcoSG",
        "outputId": "05d1bcd2-261a-498d-e225-7ec9f5d93468"
      },
      "outputs": [],
      "source": [
        "#@title Align image\n",
        "image_path = \"notebooks/images/input_img.jpg\" #@param {type: \"string\"}\n",
        "original_image = Image.open(image_path)\n",
        "original_image = original_image.convert(\"RGB\")\n",
        "if experiment_type == \"ffhq_encode\" and 'shape_predictor_68_face_landmarks.dat' not in os.listdir():\n",
        "    !wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "    !bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "def run_alignment(image_path):\n",
        "  import dlib\n",
        "  from utils.alignment import align_face\n",
        "  predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "  aligned_image = align_face(filepath=image_path, predictor=predictor) \n",
        "  print(\"Aligned image has shape: {}\".format(aligned_image.size))\n",
        "  return aligned_image \n",
        "\n",
        "if experiment_type == \"ffhq_encode\":\n",
        "  input_image = run_alignment(image_path)\n",
        "else:\n",
        "  input_image = original_image\n",
        "\n",
        "input_image.resize(resize_dims)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "QUw_UdJJc3go",
        "outputId": "a52db1a9-a6d7-47b0-f680-098ce2b005c6"
      },
      "outputs": [],
      "source": [
        "#@title Invert the image\n",
        "img_transforms = EXPERIMENT_ARGS['transform']\n",
        "transformed_image = img_transforms(input_image)\n",
        "\n",
        "def display_alongside_source_image(result_image, source_image):\n",
        "    res = np.concatenate([np.array(source_image.resize(resize_dims)),\n",
        "                          np.array(result_image.resize(resize_dims))], axis=1)\n",
        "    return Image.fromarray(res)\n",
        "\n",
        "def run_on_batch(inputs, net):\n",
        "    images, latents = net(inputs.to(\"cuda\").float(), randomize_noise=False, return_latents=True)\n",
        "    if experiment_type == 'cars_encode':\n",
        "        images = images[:, :, 32:224, :]\n",
        "    return images, latents\n",
        "\n",
        "with torch.no_grad():\n",
        "    images, latents = run_on_batch(transformed_image.unsqueeze(0), net)\n",
        "    result_image, latent = images[0], latents[0]\n",
        "#This is done in a clone git, so should download the file manually and then upload it.\n",
        "torch.save(latents, 'generated_latents.pt')\n",
        "files.download('generated_latents.pt')\n",
        "# Display inversion:\n",
        "display_alongside_source_image(tensor2im(result_image), input_image)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
